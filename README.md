Ollama Streamlit Chat Bot 
Una interfaz gr谩fica simple y efectiva para interactuar con modelos de lenguaje de Ollama usando Streamlit. Esta aplicaci贸n permite mantener conversaciones con el modelo LLaMA 2 a trav茅s de una interfaz web amigable.
 Caracter铆sticas

Interfaz web intuitiva con Streamlit
Respuestas en tiempo real (streaming)
Integraci贸n con Ollama API
Soporte para el modelo LLaMA 2 (7B)
Manejo de errores robusto
Interfaz responsiva y amigable

 Prerequisitos

Python 3.8 o superior
Ollama instalado en tu sistema
Modelo LLaMA 2 (7B) descargado en Ollama

 Instalaci贸n

Clona el repositorio
Instala las dependencias desde requirements.txt
Aseg煤rate de tener Ollama iniciado y el modelo LLaMA 2 descargado

 Uso

Inicia el servidor de Ollama
Ejecuta la aplicaci贸n Streamlit
Abre tu navegador en http://localhost:8501

 Estructura del Proyecto
Copyollama-streamlit-bot/
 app.py              # Aplicaci贸n principal
 requirements.txt    # Dependencias del proyecto
 README.md          # Este archivo
 Requirements.txt
Copystreamlit>=1.24.0
requests>=2.31.0
 Contribuciones
Las contribuciones son bienvenidas. Por favor, abre un issue primero para discutir qu茅 te gustar铆a cambiar.
锔 Soluci贸n de Problemas
Si encuentras el error "404 Client Error: Not Found", verifica:

Que Ollama est谩 corriendo
Que el modelo est谩 instalado
Que el servidor est谩 accesible en localhost:11434

 Licencia
Este proyecto est谩 bajo la Licencia MIT - ver el archivo LICENSE.md para m谩s detalles.
 charly aguiar


GitHub: Charlesagui


猸锔 Muestra tu Apoyo
Si este proyecto te ha sido 煤til, considera darle una estrella en GitHub.
 Notas

Esta aplicaci贸n est谩 dise帽ada para uso local y educativo
El rendimiento depender谩 de tu hardware
Se recomienda tener al menos 16GB de RAM para un funcionamiento 贸ptimo